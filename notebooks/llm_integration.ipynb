{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7931d65",
   "metadata": {},
   "source": [
    "## LLM Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940748c4",
   "metadata": {},
   "source": [
    "Once all the steps were developed:\n",
    "\n",
    "- Embedding Service\n",
    "- Ingestion Pipeline\n",
    "- Context retrieval\n",
    "\n",
    "Now its time to create the last part of the RAG-LLM technique. Send the context and the user's query to the LLM in order to the LLM to generate an answer.\n",
    "\n",
    "This time, I'll be using ChatGPT LLM's, but also can work with Google LLM and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cde1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from rag_llm_energy_expert.credentials import get_qdrant_config, get_llm_config\n",
    "from rag_llm_energy_expert.search.searchers import semantic_search\n",
    "from rag_llm_energy_expert.llm.chat import create_chat_session, generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea5ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_config=get_qdrant_config()\n",
    "llm_config=get_llm_config()\n",
    "collection_name = qdrant_config.COLLECTION_NAME + qdrant_config.COLLECTION_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8a07d",
   "metadata": {},
   "source": [
    "## Connecting the GENAI client\n",
    "\n",
    "code from: https://ai.google.dev/gemini-api/docs/text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb4c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = genai.Client(api_key=llm_config.API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f1fc1",
   "metadata": {},
   "source": [
    "Generating multi-turn conversations\n",
    "\n",
    "The chat format enables users to step incrementally toward answers and to get help with multipart problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51747ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat = llm_client.chats.create(model=llm_config.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4adb3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay! Is there anything specific you'd like to talk about, or anything I can help you with related to being 25? I can offer information or assistance on topics like:\n",
      "\n",
      "*   **Career Advice:** Exploring job opportunities, resume tips, interview preparation.\n",
      "*   **Financial Planning:** Budgeting, saving, investing, managing debt.\n",
      "*   **Health and Wellness:** Fitness, nutrition, mental health resources.\n",
      "*   **Relationships:** Dating, friendships, family dynamics.\n",
      "*   **Travel and Leisure:** Planning trips, finding activities.\n",
      "*   **General Knowledge:** Answering questions on a wide range of topics.\n",
      "*   **Entertainment:** Recommending books, movies, music, games.\n",
      "\n",
      "Just let me know what's on your mind!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Hi, im 25 years old\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9ae16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To figure out your sister's age, you need to subtract 5 years from your age. Since you are 25 years old:\\n\\n25 - 5 = 20\\n\\nTherefore, your sister is **20 years old**.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"If I am 5 years older than my sister. How old is she?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c284c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role - user: Hi, im 25 years old\n",
      "role - model: Okay! Is there anything specific you'd like to talk about, or anything I can help you with related to being 25? I can offer information or assistance on topics like:\n",
      "\n",
      "*   **Career Advice:** Exploring job opportunities, resume tips, interview preparation.\n",
      "*   **Financial Planning:** Budgeting, saving, investing, managing debt.\n",
      "*   **Health and Wellness:** Fitness, nutrition, mental health resources.\n",
      "*   **Relationships:** Dating, friendships, family dynamics.\n",
      "*   **Travel and Leisure:** Planning trips, finding activities.\n",
      "*   **General Knowledge:** Answering questions on a wide range of topics.\n",
      "*   **Entertainment:** Recommending books, movies, music, games.\n",
      "\n",
      "Just let me know what's on your mind!\n",
      "\n",
      "role - user: If I am 5 years older than my sister. How old is she?\n",
      "role - model: To figure out your sister's age, you need to subtract 5 years from your age. Since you are 25 years old:\n",
      "\n",
      "25 - 5 = 20\n",
      "\n",
      "Therefore, your sister is **20 years old**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113b287",
   "metadata": {},
   "source": [
    "### Configuring parameters\n",
    "\n",
    "Every prompt sent to the model includes parameters that control how the model generates responses. You can configure these parameters, por let the model use the default options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00538a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat2 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a742326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "*   **Cost of apples:** 5 apples * $1/apple = $5\n",
      "*   **Cost of pears:** $10 (total) - $5 (apples) = $5\n",
      "*   **Cost per pear:** $5 / 2 pears = $2.50/pear\n",
      "\n",
      "**Answer:** The pears cost $2.50 each.\n"
     ]
    }
   ],
   "source": [
    "responses = chat2.send_message(\n",
    "    message = \"Hi, If I have 5 apples, and 2 pears, and for all of them I paid 10 USD, if the apples costs 1 USD, how much are the pears?\"\n",
    ")\n",
    "print(responses.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a96c8",
   "metadata": {},
   "source": [
    "More model parameters can be found [here](https://ai.google.dev/gemini-api/docs/text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116e76",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "\n",
    "System instructions let you steer the behaviour of a model baesd on you specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full iteraction with the user, enabling you to specify product-level behaviour separete from the prompts provided by end users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2641ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat3 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1,\n",
    "        system_instruction=\"You are a Mexican energy expert that solves doubts of clients. You must be as direct as possible. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked.\"\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc09f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"En el nuevo modelo, cómo se considera a Pemex?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81305df",
   "metadata": {},
   "source": [
    "Semantic Search of the available info in the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5555a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:34:55.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:34:55.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:03.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:03.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:03.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:04.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:04.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "adjudicado el contrato. Cuando una asignación migre a un contrato podrá llevarse a cabo una \n",
      "asociación entre Pemex y un partícular, y la CNH realizará una licitación para elegir al socio \n",
      "(incluyendo las asignaciones de la Ronda Cero).\n",
      "• El modelo propuesto también considera que Pemex podrá migrar a la nueva modalidad de \n",
      "contratación\n",
      "\n",
      " Pemex en la industria petrolera. Mediante \n",
      "la “Ronda Cero”, Pemex podrá elegir aquellos campos en producción y aquellas áreas en ex-\n",
      "ploración que tengan interés en operar y donde demuestre tener capacidad técnica, financie-\n",
      "ra y de ejecución para desarrollarlos en forma eficiente y competitiva y podrá migrarlas hacia \n",
      "un esqu\n",
      "\n",
      " pensiones y jubilaciones \n",
      "de PEMEX y CFE, sujeto a que acuerden con sus trabajadores un nuevo régimen de pensiones \n",
      "que reduzca esos pasivos y la Auditoria Superior de la Federación audite la evolución de di-\n",
      "chos pasivos.\n",
      "• Los Consejos de Administración de ambas empresas tendrán una nueva estructura organi-\n",
      "zacional y se encargarán de: i) Definir\n",
      "\n",
      "ción de hidrocarburos el Estado \n",
      "tiene la posibilidad de otorgar asignaciones o suscribir contratos.\n",
      "\n",
      "6\n",
      "Asignaciones \n",
      "• Se otorgarán a Pemex en la Ronda Cero.\n",
      "• Posteriormente, se otorgan de forma excepcional a Pemex y a otras \n",
      "Empresas Productivas del Estado.\n",
      "• Las asignaciones permiten la adjudicación directa a Pemex de pro-\n",
      "yectos est\n",
      "\n",
      " la participación de particulares en refinación y procesamiento de gas natural, pre-\n",
      "cio permiso de SENER pudiendo operar por su cuenta o en asociación con Empresas Produc-\n",
      "tivas del Estado. Pemex también podrá asociarse, como actualmente ya lo hace en el extran-\n",
      "jero.\n",
      "• Se establece la petroquímica en todas sus fases como actividad de libre concurrencia.\n",
      "•\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(semantic_search(\n",
    "    query=query,\n",
    "    embedding_model_name=None,\n",
    "    chunk_overlap=0,\n",
    "    documents_limit=5,\n",
    "    collection_name=collection_name\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "093d3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:35:04.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:04.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:17.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:17.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:18.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:18.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:18.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Sí, Pemex tiene la opción de migrar a la nueva modalidad de contratación. Esto le permite elegir campos en producción y áreas de exploración donde demuestre tener la capacidad técnica y financiera para desarrollarlos de manera eficiente. ¡Es una gran oportunidad para Pemex!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"PEMEX podrá migrar a la nueva modalidad de contratación\"\n",
    "\n",
    "response = chat3.send_message(message=question,\n",
    "                              config = types.GenerateContentConfig(\n",
    "                                  temperature=0.5,\n",
    "                                  system_instruction=\"You are a Mexican energy expert that solves doubts of clients. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked. Try to generate friendly answers\"\\\n",
    "        f\"\"\"Context: {semantic_search(query=question,\n",
    "                                    embedding_model_name=None,\n",
    "                                    chunk_overlap=0,\n",
    "                                    collection_name = collection_name,\n",
    "                                    documents_limit = 5\n",
    "                                    )}\"\"\"\n",
    "                              ))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0704abb",
   "metadata": {},
   "source": [
    "## Using the functions generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf887175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:35:19.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mcreate_chat_session\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mCreating a new chat session...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mcreate_chat_session\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mChat session created successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat = create_chat_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "729a23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:35:19.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating response...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mRetrieving context...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:23.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:23.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:23.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mContext retrieved successfully.\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mGenerating response...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mResponse generated successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí, Pemex podrá migrar a la nueva modalidad de contratación.\n"
     ]
    }
   ],
   "source": [
    "query=\"PEMEX podrá migrar a la nueva modalidad de contratación\"\n",
    "\n",
    "print(generate_response(prompt=query, chat_session=chat, temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249a566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
