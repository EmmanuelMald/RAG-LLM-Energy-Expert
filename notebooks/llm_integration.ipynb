{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7931d65",
   "metadata": {},
   "source": [
    "## LLM Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940748c4",
   "metadata": {},
   "source": [
    "Once all the steps were developed:\n",
    "\n",
    "- Embedding Service\n",
    "- Ingestion Pipeline\n",
    "- Context retrieval\n",
    "\n",
    "Now its time to create the last part of the RAG-LLM technique. Send the context and the user's query to the LLM in order to the LLM to generate an answer.\n",
    "\n",
    "This time, I'll be using ChatGPT LLM's, but also can work with Google LLM and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cde1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from rag_llm_energy_expert.credentials import get_qdrant_config, get_llm_config\n",
    "from rag_llm_energy_expert.search.searchers import semantic_search\n",
    "from rag_llm_energy_expert.llm.chat import create_chat_session, generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea5ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_config=get_qdrant_config()\n",
    "llm_config=get_llm_config()\n",
    "collection_name = qdrant_config.COLLECTION_NAME + qdrant_config.COLLECTION_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8a07d",
   "metadata": {},
   "source": [
    "## Connecting the GENAI client\n",
    "\n",
    "code from: https://ai.google.dev/gemini-api/docs/text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeb4c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = genai.Client(api_key=llm_config.API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f1fc1",
   "metadata": {},
   "source": [
    "Generating multi-turn conversations\n",
    "\n",
    "The chat format enables users to step incrementally toward answers and to get help with multipart problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51747ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat = llm_client.chats.create(model=llm_config.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4adb3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay! That's great. How can I help you today? Are you looking for information about something specific, or just looking to chat? Tell me what's on your mind.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Hi, im 25 years old\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa9ae16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Since you are 25 years old and 5 years older than your sister, she is 25 - 5 = 20 years old.\\n\\nSo, your sister is **20 years old**.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"If I am 5 years older than my sister. How old is she?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c284c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role - user: Hi, im 25 years old\n",
      "role - model: Okay! That's great. How can I help you today? Are you looking for information about something specific, or just looking to chat? Tell me what's on your mind.\n",
      "\n",
      "role - user: If I am 5 years older than my sister. How old is she?\n",
      "role - model: Since you are 25 years old and 5 years older than your sister, she is 25 - 5 = 20 years old.\n",
      "\n",
      "So, your sister is **20 years old**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113b287",
   "metadata": {},
   "source": [
    "### Configuring parameters\n",
    "\n",
    "Every prompt sent to the model includes parameters that control how the model generates responses. You can configure these parameters, por let the model use the default options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00538a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat2 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a742326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "*   **Cost of apples:** 5 apples * $1/apple = $5\n",
      "*   **Cost of pears:** $10 (total) - $5 (apples) = $5\n",
      "*   **Cost per pear:** $5 / 2 pears = $2.50/pear\n",
      "\n",
      "**Answer:** The pears cost $2.50 each.\n"
     ]
    }
   ],
   "source": [
    "responses = chat2.send_message(\n",
    "    message = \"Hi, If I have 5 apples, and 2 pears, and for all of them I paid 10 USD, if the apples costs 1 USD, how much are the pears?\"\n",
    ")\n",
    "print(responses.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a96c8",
   "metadata": {},
   "source": [
    "More model parameters can be found [here](https://ai.google.dev/gemini-api/docs/text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116e76",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "\n",
    "System instructions let you steer the behaviour of a model baesd on you specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full iteraction with the user, enabling you to specify product-level behaviour separete from the prompts provided by end users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2641ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat3 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1,\n",
    "        system_instruction=\"You are a Mexican energy expert that solves doubts of clients. You must be as direct as possible. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked.\"\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc09f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"En el nuevo modelo, c√≥mo se considera a Pemex?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81305df",
   "metadata": {},
   "source": [
    "Semantic Search of the available info in the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5555a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-22 10:42:53.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:53.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:55.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:55.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:55.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:56.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:56.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pemex y la CFE se mantienen como empresas 100% mexicanas y 100% del Estado, cuya organizaci√≥n, administraci√≥n, organizaci√≥n y estructura corporativa ser√°n acordes con las mejores pr√°cticas internacionales.  \n",
      "**Legislaci√≥n Secundaria**  \n",
      "- Pemex y CFE contar√°n con un r√©gimen presupuestario especial y exclusivo que les otorga autonom√≠a presupuestaria y las sujeta s√≥lo al balance financiero y al techo de servicios personales.  \n",
      "- Para el manejo de su deuda, Pemex y CFE se regir√°n por lo dispuesto en un art√≠culo especial de la Ley General de Deuda P√∫blica. Ambas empresas podr√°n realizar negociaciones, as√≠\n",
      "como contratar los financiamientos externos e internos que requieran, para lo que deber√°n\n",
      "coordinarse con la SHCP, sin requerir de su autorizaci√≥n.  \n",
      "- El Gobierno de la Rep√∫blica dejar√° de ser el ‚Äúadministrador principal‚Äù de los √≥rganos de gobierno de las Empresas Productivas del Estado para asumir el rol de propietario.\n",
      "\n",
      "- Toda la informaci√≥n geol√≥gica del pa√≠s ser√° entregada a CNH, quien la administrar√° en el Centro Nacional de Informaci√≥n de Hidrocarburos. Pemex y las empresas productivas del Estado\n",
      "y los particulares podr√°n realizar estudios de reconocimiento o exploraci√≥n superficial, previa\n",
      "autorizaci√≥n de CNH.  \n",
      "- La legislaci√≥n secundaria contempla que Pemex o los particulares podr√°n proponer a la SENER √°reas a licitar en el futuro, con base en la informaci√≥n obtenida. Las propuestas no otorgan derechos ni ventajas para la suscripci√≥n de un contrato.\n",
      "\n",
      "### Asignaciones  \n",
      "- Se otorgar√°n a Pemex en la Ronda Cero.  \n",
      "- Posteriormente, se otorgan de forma excepcional a Pemex y a otras  \n",
      "Empresas Productivas del Estado.  \n",
      "- Las asignaciones permiten la adjudicaci√≥n directa a Pemex de proyectos estrat√©gicos tales como yacimientos transfronterizos.  \n",
      "- Los asignatarios podr√°n registrar el beneficio econ√≥mico esperado  \n",
      "para efectos financieros y contables.\n",
      "\n",
      "### 19  \n",
      "-----  \n",
      "**Caracter√≠sticas del nuevo modelo**  \n",
      "- En el Art√≠culo 25 Constitucional se plasma el principio de sustentabilidad como uno de los criterios para el desarrollo de los proyectos de infraestructura energ√©tica.  \n",
      "- El Ejecutivo Federal deber√° incluir en el Programa Nacional para el Aprovechamiento Sustentable de la Energ√≠a los pasos a seguir, as√≠ como las condiciones de operaci√≥n y financiamiento\n",
      "para promover el uso de tecnolog√≠as y combustibles m√°s limpios.  \n",
      "- En el sector el√©ctrico se establecieron obligaciones para el uso de energ√≠as limpias y la reducci√≥n de emisiones contaminantes.  \n",
      "- Se crea la Agencia Nacional de Seguridad Industrial y Protecci√≥n al Medio Ambiente del Sector Hidrocarburos. √âsta regular√° la seguridad industrial para minimizar el riesgo de accidentes\n",
      "en instalaciones o afectaciones al medio ambiente causadas por la actividad petrolera.  \n",
      "**Legislaci√≥n Secundaria**  \n",
      "- La Agencia Nacional de Seguridad Industrial y Protecci√≥n al Medio Ambiente del Sector Hidrocarburos ser√° un √≥rgano desconcentrado de la Secretar√≠a de Medio Ambiente y Recursos\n",
      "Naturales (SEMARNAT), especializado t√©cnicamente y que contar√° con autonom√≠a de\n",
      "gesti√≥n.  \n",
      "- Se propone que dicha Agencia tenga las atribuciones de regular, supervisar y sancionar en\n",
      "\n",
      "## III. Fondo Mexicano del Petr√≥leo para la Estabilizaci√≥n y el Desarrollo  \n",
      "Durante los √∫ltimos 30 a√±os, la industria petrolera y sus ingresos han sido el pilar de las finanzas p√∫blicas y el motor de la actividad econ√≥mica del pa√≠s. Hoy, el principal reto a las finanzas\n",
      "p√∫blicas es revertir la ca√≠da en la producci√≥n de petr√≥leo. Hasta ahora, los altos precios de dicho\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(semantic_search(\n",
    "    query=query,\n",
    "    embedding_model_name=None,\n",
    "    chunk_overlap=0,\n",
    "    documents_limit=5,\n",
    "    collection_name=collection_name\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "093d3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-22 10:42:56.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:56.256\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:57.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:57.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:57.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:57.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:57.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! S√≠, Pemex tiene la posibilidad de migrar a la nueva modalidad de contrataci√≥n. Esto le permitir√° poner en producci√≥n yacimientos de hidrocarburos que actualmente no se explotan por falta de inversi√≥n, capacidad de ejecuci√≥n y tecnolog√≠a.\n"
     ]
    }
   ],
   "source": [
    "question = \"PEMEX podr√° migrar a la nueva modalidad de contrataci√≥n\"\n",
    "\n",
    "response = chat3.send_message(message=question,\n",
    "                              config = types.GenerateContentConfig(\n",
    "                                  temperature=0.5,\n",
    "                                  system_instruction=\"You are a Mexican energy expert that solves doubts of clients. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked. Try to generate friendly answers\"\\\n",
    "        f\"\"\"Context: {semantic_search(query=question,\n",
    "                                    embedding_model_name=None,\n",
    "                                    chunk_overlap=0,\n",
    "                                    collection_name = collection_name,\n",
    "                                    documents_limit = 5\n",
    "                                    )}\"\"\"\n",
    "                              ))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0704abb",
   "metadata": {},
   "source": [
    "## Using the functions generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf887175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-22 10:42:58.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mcreate_chat_session\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mCreating a new chat session...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:58.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mcreate_chat_session\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mChat session created successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat = create_chat_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "729a23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-22 10:42:58.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating response...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:58.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mRetrieving context...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:58.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:42:58.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:00.089\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:00.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:00.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:00.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:00.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:00.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mContext retrieved successfully.\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:00.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mGenerating response...\u001b[0m\n",
      "\u001b[32m2025-04-22 10:43:01.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mResponse generated successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S√≠, se establece la posibilidad de que la Naci√≥n otorgue asignaciones o contratos a Pemex, lo que le permitir√≠a poner en producci√≥n yacimientos de hidrocarburos que actualmente est√°n ociosos por falta de inversi√≥n, capacidad de ejecuci√≥n y tecnolog√≠a.\n"
     ]
    }
   ],
   "source": [
    "query=\"PEMEX podr√° migrar a la nueva modalidad de contrataci√≥n\"\n",
    "\n",
    "print(generate_response(prompt=query, chat_session=chat, temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249a566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
