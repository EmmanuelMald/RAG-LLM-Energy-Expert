{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7931d65",
   "metadata": {},
   "source": [
    "## LLM Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940748c4",
   "metadata": {},
   "source": [
    "Once all the steps were developed:\n",
    "\n",
    "- Embedding Service\n",
    "- Ingestion Pipeline\n",
    "- Context retrieval\n",
    "\n",
    "Now its time to create the last part of the RAG-LLM technique. Send the context and the user's query to the LLM in order to the LLM to generate an answer.\n",
    "\n",
    "This time, I'll be using ChatGPT LLM's, but also can work with Google LLM and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cde1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from rag_llm_energy_expert.credentials import get_qdrant_config, get_llm_config\n",
    "from rag_llm_energy_expert.search.searchers import semantic_search\n",
    "from rag_llm_energy_expert.llm.chat import create_chat_session, generate_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ea5ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_config=get_qdrant_config()\n",
    "llm_config=get_llm_config()\n",
    "collection_name = qdrant_config.COLLECTION_NAME + qdrant_config.COLLECTION_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8a07d",
   "metadata": {},
   "source": [
    "## Connecting the GENAI client\n",
    "\n",
    "code from: https://ai.google.dev/gemini-api/docs/text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb4c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = genai.Client(api_key=llm_config.API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f1fc1",
   "metadata": {},
   "source": [
    "Generating multi-turn conversations\n",
    "\n",
    "The chat format enables users to step incrementally toward answers and to get help with multipart problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51747ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat = llm_client.chats.create(model=llm_config.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4adb3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay! Is there anything specific you'd like to talk about, or anything I can help you with related to being 25? I can offer information or assistance on topics like:\n",
      "\n",
      "*   **Career Advice:** Exploring job opportunities, resume tips, interview preparation.\n",
      "*   **Financial Planning:** Budgeting, saving, investing, managing debt.\n",
      "*   **Health and Wellness:** Fitness, nutrition, mental health resources.\n",
      "*   **Relationships:** Dating, friendships, family dynamics.\n",
      "*   **Travel and Leisure:** Planning trips, finding activities.\n",
      "*   **General Knowledge:** Answering questions on a wide range of topics.\n",
      "*   **Entertainment:** Recommending books, movies, music, games.\n",
      "\n",
      "Just let me know what's on your mind!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Hi, im 25 years old\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9ae16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To figure out your sister's age, you need to subtract 5 years from your age. Since you are 25 years old:\\n\\n25 - 5 = 20\\n\\nTherefore, your sister is **20 years old**.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"If I am 5 years older than my sister. How old is she?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c284c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role - user: Hi, im 25 years old\n",
      "role - model: Okay! Is there anything specific you'd like to talk about, or anything I can help you with related to being 25? I can offer information or assistance on topics like:\n",
      "\n",
      "*   **Career Advice:** Exploring job opportunities, resume tips, interview preparation.\n",
      "*   **Financial Planning:** Budgeting, saving, investing, managing debt.\n",
      "*   **Health and Wellness:** Fitness, nutrition, mental health resources.\n",
      "*   **Relationships:** Dating, friendships, family dynamics.\n",
      "*   **Travel and Leisure:** Planning trips, finding activities.\n",
      "*   **General Knowledge:** Answering questions on a wide range of topics.\n",
      "*   **Entertainment:** Recommending books, movies, music, games.\n",
      "\n",
      "Just let me know what's on your mind!\n",
      "\n",
      "role - user: If I am 5 years older than my sister. How old is she?\n",
      "role - model: To figure out your sister's age, you need to subtract 5 years from your age. Since you are 25 years old:\n",
      "\n",
      "25 - 5 = 20\n",
      "\n",
      "Therefore, your sister is **20 years old**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113b287",
   "metadata": {},
   "source": [
    "### Configuring parameters\n",
    "\n",
    "Every prompt sent to the model includes parameters that control how the model generates responses. You can configure these parameters, por let the model use the default options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00538a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat2 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a742326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "*   **Cost of apples:** 5 apples * $1/apple = $5\n",
      "*   **Cost of pears:** $10 (total) - $5 (apples) = $5\n",
      "*   **Cost per pear:** $5 / 2 pears = $2.50/pear\n",
      "\n",
      "**Answer:** The pears cost $2.50 each.\n"
     ]
    }
   ],
   "source": [
    "responses = chat2.send_message(\n",
    "    message = \"Hi, If I have 5 apples, and 2 pears, and for all of them I paid 10 USD, if the apples costs 1 USD, how much are the pears?\"\n",
    ")\n",
    "print(responses.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a96c8",
   "metadata": {},
   "source": [
    "More model parameters can be found [here](https://ai.google.dev/gemini-api/docs/text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116e76",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "\n",
    "System instructions let you steer the behaviour of a model baesd on you specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full iteraction with the user, enabling you to specify product-level behaviour separete from the prompts provided by end users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2641ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat3 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1,\n",
    "        system_instruction=\"You are a Mexican energy expert that solves doubts of clients. You must be as direct as possible. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked.\"\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc09f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"En el nuevo modelo, c√≥mo se considera a Pemex?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81305df",
   "metadata": {},
   "source": [
    "Semantic Search of the available info in the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5555a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:34:55.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:34:55.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:03.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:03.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:03.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:04.058\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:04.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "adjudicado el contrato. Cuando una asignaci√≥n migre a un contrato podr√° llevarse a cabo una \n",
      "asociaci√≥n entre Pemex y un part√≠cular, y la CNH realizar√° una licitaci√≥n para elegir al socio \n",
      "(incluyendo las asignaciones de la Ronda Cero).\n",
      "‚Ä¢ El modelo propuesto tambi√©n considera que Pemex podr√° migrar a la nueva modalidad de \n",
      "contrataci√≥n\n",
      "\n",
      " Pemex en la industria petrolera. Mediante \n",
      "la ‚ÄúRonda Cero‚Äù, Pemex podr√° elegir aquellos campos en producci√≥n y aquellas √°reas en ex-\n",
      "ploraci√≥n que tengan inter√©s en operar y donde demuestre tener capacidad t√©cnica, financie-\n",
      "ra y de ejecuci√≥n para desarrollarlos en forma eficiente y competitiva y podr√° migrarlas hacia \n",
      "un esqu\n",
      "\n",
      " pensiones y jubilaciones \n",
      "de PEMEX y CFE, sujeto a que acuerden con sus trabajadores un nuevo r√©gimen de pensiones \n",
      "que reduzca esos pasivos y la Auditoria Superior de la Federaci√≥n audite la evoluci√≥n de di-\n",
      "chos pasivos.\n",
      "‚Ä¢ Los Consejos de Administraci√≥n de ambas empresas tendr√°n una nueva estructura organi-\n",
      "zacional y se encargar√°n de: i) Definir\n",
      "\n",
      "ci√≥n de hidrocarburos el Estado \n",
      "tiene la posibilidad de otorgar asignaciones o suscribir contratos.\n",
      "\n",
      "6\n",
      "Asignaciones \n",
      "‚Ä¢ Se otorgar√°n a Pemex en la Ronda Cero.\n",
      "‚Ä¢ Posteriormente, se otorgan de forma excepcional a Pemex y a otras \n",
      "Empresas Productivas del Estado.\n",
      "‚Ä¢ Las asignaciones permiten la adjudicaci√≥n directa a Pemex de pro-\n",
      "yectos est\n",
      "\n",
      " la participaci√≥n de particulares en refinaci√≥n y procesamiento de gas natural, pre-\n",
      "cio permiso de SENER pudiendo operar por su cuenta o en asociaci√≥n con Empresas Produc-\n",
      "tivas del Estado. Pemex tambi√©n podr√° asociarse, como actualmente ya lo hace en el extran-\n",
      "jero.\n",
      "‚Ä¢ Se establece la petroqu√≠mica en todas sus fases como actividad de libre concurrencia.\n",
      "‚Ä¢\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(semantic_search(\n",
    "    query=query,\n",
    "    embedding_model_name=None,\n",
    "    chunk_overlap=0,\n",
    "    documents_limit=5,\n",
    "    collection_name=collection_name\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "093d3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:35:04.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:04.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:17.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:17.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:18.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:18.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:18.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! S√≠, Pemex tiene la opci√≥n de migrar a la nueva modalidad de contrataci√≥n. Esto le permite elegir campos en producci√≥n y √°reas de exploraci√≥n donde demuestre tener la capacidad t√©cnica y financiera para desarrollarlos de manera eficiente. ¬°Es una gran oportunidad para Pemex!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"PEMEX podr√° migrar a la nueva modalidad de contrataci√≥n\"\n",
    "\n",
    "response = chat3.send_message(message=question,\n",
    "                              config = types.GenerateContentConfig(\n",
    "                                  temperature=0.5,\n",
    "                                  system_instruction=\"You are a Mexican energy expert that solves doubts of clients. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked. Try to generate friendly answers\"\\\n",
    "        f\"\"\"Context: {semantic_search(query=question,\n",
    "                                    embedding_model_name=None,\n",
    "                                    chunk_overlap=0,\n",
    "                                    collection_name = collection_name,\n",
    "                                    documents_limit = 5\n",
    "                                    )}\"\"\"\n",
    "                              ))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0704abb",
   "metadata": {},
   "source": [
    "## Using the functions generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf887175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:35:19.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mcreate_chat_session\u001b[0m:\u001b[36m42\u001b[0m - \u001b[1mCreating a new chat session...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mcreate_chat_session\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mChat session created successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat = create_chat_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "729a23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 22:35:19.491\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating response...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mRetrieving context...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:19.503\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:23.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:23.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:23.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mContext retrieved successfully.\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mGenerating response...\u001b[0m\n",
      "\u001b[32m2025-04-18 22:35:24.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.llm.chat\u001b[0m:\u001b[36mgenerate_response\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mResponse generated successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S√≠, Pemex podr√° migrar a la nueva modalidad de contrataci√≥n.\n"
     ]
    }
   ],
   "source": [
    "query=\"PEMEX podr√° migrar a la nueva modalidad de contrataci√≥n\"\n",
    "\n",
    "print(generate_response(prompt=query, chat_session=chat, temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249a566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
