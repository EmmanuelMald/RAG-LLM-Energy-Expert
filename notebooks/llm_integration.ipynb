{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7931d65",
   "metadata": {},
   "source": [
    "## LLM Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940748c4",
   "metadata": {},
   "source": [
    "Once all the steps were developed:\n",
    "\n",
    "- Embedding Service\n",
    "- Ingestion Pipeline\n",
    "- Context retrieval\n",
    "\n",
    "Now its time to create the last part of the RAG-LLM technique. Send the context and the user's query to the LLM in order to the LLM to generate an answer.\n",
    "\n",
    "This time, I'll be using ChatGPT LLM's, but also can work with Google LLM and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cde1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17200d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_llm_energy_expert.credentials import get_qdrant_config, get_llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f0d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_llm_energy_expert.search.searchers import semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea5ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_config=get_qdrant_config()\n",
    "llm_config=get_llm_config()\n",
    "collection_name = qdrant_config.COLLECTION_NAME + qdrant_config.COLLECTION_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8a07d",
   "metadata": {},
   "source": [
    "## Connecting the GENAI client\n",
    "\n",
    "code from: https://ai.google.dev/gemini-api/docs/text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb4c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = genai.Client(api_key=llm_config.API_KEY.get_secret_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f1fc1",
   "metadata": {},
   "source": [
    "Generating multi-turn conversations\n",
    "\n",
    "The chat format enables users to step incrementally toward answers and to get help with multipart problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51747ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat = llm_client.chats.create(model=llm_config.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4adb3603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay! That's great. Is there anything specific you'd like to talk about or need help with? Knowing you're 25 doesn't give me much context, but I'm happy to help in any way I can. For example, are you looking for:\n",
      "\n",
      "*   **Advice on something specific?** (career, relationships, finances, etc.)\n",
      "*   **Information about something?** (hobbies, travel, current events, etc.)\n",
      "*   **Ideas for something?** (gifts, activities, etc.)\n",
      "*   **Just someone to chat with?**\n",
      "\n",
      "Let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Hi, im 25 years old\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa9ae16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You can't determine your sister's exact age with just that information. You only know that she is 5 years younger than you.\\n\\nSince you are 25, your sister is 20 years old.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message(\"If I am 5 years older than my sister. How old is she?\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c284c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role - user: Hi, tell me a joke\n",
      "role - model: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "role - user: Hi, tell me a joke\n",
      "role - model: Why did the bicycle fall over? \n",
      "\n",
      "Because it was two tired!\n",
      "\n",
      "role - user: Hi, im 25 years old\n",
      "role - model: Okay! That's great. Is there anything specific you'd like to talk about or need help with? Knowing you're 25 doesn't give me much context, but I'm happy to help in any way I can. For example, are you looking for:\n",
      "\n",
      "*   **Advice on something specific?** (career, relationships, finances, etc.)\n",
      "*   **Information about something?** (hobbies, travel, current events, etc.)\n",
      "*   **Ideas for something?** (gifts, activities, etc.)\n",
      "*   **Just someone to chat with?**\n",
      "\n",
      "Let me know!\n",
      "\n",
      "role - user: If I am 5 years older than my sister. How old is she?\n",
      "role - model: You can't determine your sister's exact age with just that information. You only know that she is 5 years younger than you.\n",
      "\n",
      "Since you are 25, your sister is 20 years old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113b287",
   "metadata": {},
   "source": [
    "### Configuring parameters\n",
    "\n",
    "Every prompt sent to the model includes parameters that control how the model generates responses. You can configure these parameters, por let the model use the default options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00538a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat2 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a742326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "*   **Cost of apples:** 5 apples * $1/apple = $5\n",
      "*   **Cost of pears:** $10 (total) - $5 (apples) = $5\n",
      "*   **Cost per pear:** $5 / 2 pears = $2.50/pear\n",
      "\n",
      "**Answer:** The pears cost $2.50 each.\n"
     ]
    }
   ],
   "source": [
    "responses = chat2.send_message(\n",
    "    message = \"Hi, If I have 5 apples, and 2 pears, and for all of them I paid 10 USD, if the apples costs 1 USD, how much are the pears?\"\n",
    ")\n",
    "print(responses.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a96c8",
   "metadata": {},
   "source": [
    "More model parameters can be found [here](https://ai.google.dev/gemini-api/docs/text-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116e76",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "\n",
    "System instructions let you steer the behaviour of a model baesd on you specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full iteraction with the user, enabling you to specify product-level behaviour separete from the prompts provided by end users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2641ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new chat session\n",
    "chat3 = llm_client.chats.create(\n",
    "    model=llm_config.MODEL,\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,\n",
    "        temperature=0.1,\n",
    "        system_instruction=\"You are a Mexican energy expert that solves doubts of clients. You must be as direct as possible. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked.\"\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc09f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"En el nuevo modelo, cómo se considera a Pemex?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81305df",
   "metadata": {},
   "source": [
    "Semantic Search of the available info in the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5555a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 19:12:19.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 19:12:19.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 19:12:24.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 19:12:24.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 19:12:24.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 19:12:24.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 19:12:24.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "adjudicado el contrato. Cuando una asignación migre a un contrato podrá llevarse a cabo una \n",
      "asociación entre Pemex y un partícular, y la CNH realizará una licitación para elegir al socio \n",
      "(incluyendo las asignaciones de la Ronda Cero).\n",
      "• El modelo propuesto también considera que Pemex podrá migrar a la nueva modalidad de \n",
      "contratación\n",
      "\n",
      " Pemex en la industria petrolera. Mediante \n",
      "la “Ronda Cero”, Pemex podrá elegir aquellos campos en producción y aquellas áreas en ex-\n",
      "ploración que tengan interés en operar y donde demuestre tener capacidad técnica, financie-\n",
      "ra y de ejecución para desarrollarlos en forma eficiente y competitiva y podrá migrarlas hacia \n",
      "un esqu\n",
      "\n",
      " pensiones y jubilaciones \n",
      "de PEMEX y CFE, sujeto a que acuerden con sus trabajadores un nuevo régimen de pensiones \n",
      "que reduzca esos pasivos y la Auditoria Superior de la Federación audite la evolución de di-\n",
      "chos pasivos.\n",
      "• Los Consejos de Administración de ambas empresas tendrán una nueva estructura organi-\n",
      "zacional y se encargarán de: i) Definir\n",
      "\n",
      "ción de hidrocarburos el Estado \n",
      "tiene la posibilidad de otorgar asignaciones o suscribir contratos.\n",
      "\n",
      "6\n",
      "Asignaciones \n",
      "• Se otorgarán a Pemex en la Ronda Cero.\n",
      "• Posteriormente, se otorgan de forma excepcional a Pemex y a otras \n",
      "Empresas Productivas del Estado.\n",
      "• Las asignaciones permiten la adjudicación directa a Pemex de pro-\n",
      "yectos est\n",
      "\n",
      " la participación de particulares en refinación y procesamiento de gas natural, pre-\n",
      "cio permiso de SENER pudiendo operar por su cuenta o en asociación con Empresas Produc-\n",
      "tivas del Estado. Pemex también podrá asociarse, como actualmente ya lo hace en el extran-\n",
      "jero.\n",
      "• Se establece la petroquímica en todas sus fases como actividad de libre concurrencia.\n",
      "•\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(semantic_search(\n",
    "    query=query,\n",
    "    embedding_model_name=None,\n",
    "    chunk_overlap=0,\n",
    "    documents_limit=5,\n",
    "    collection_name=collection_name\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "093d3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 19:11:21.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mPreprocessing query...\u001b[0m\n",
      "\u001b[32m2025-04-18 19:11:21.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mGenerating embeddings...\u001b[0m\n",
      "\u001b[32m2025-04-18 19:11:26.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mEmbeddings generated successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 19:11:26.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mPreparing embeddings for vector search\u001b[0m\n",
      "\u001b[32m2025-04-18 19:11:26.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mQuery preprocessed successfully\u001b[0m\n",
      "\u001b[32m2025-04-18 19:11:26.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mProcessing query results...\u001b[0m\n",
      "\u001b[32m2025-04-18 19:11:26.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrag_llm_energy_expert.search.searchers_auxiliars\u001b[0m:\u001b[36mprocess_query_results\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mQuery results processed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Sí, Pemex podrá migrar a la nueva modalidad de contratación. Esto le permitirá asociarse con particulares, incluso en asignaciones de la Ronda Cero, mediante licitaciones supervisadas por la CNH para elegir al socio más adecuado.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"PEMEX podrá migrar a la nueva modalidad de contratación\"\n",
    "\n",
    "response = chat3.send_message(message=question,\n",
    "                              config = types.GenerateContentConfig(\n",
    "                                  temperature=0.5,\n",
    "                                  system_instruction=\"You are a Mexican energy expert that solves doubts of clients. Your responses\" \\\n",
    "        \"shall not be longer than 2 paragraphs (5 lines each).\" \\\n",
    "        \"The responses shall be based on the context provided. If you don't know the answer, tell that you don't know.\" \\\n",
    "        \"Answer the user's questions in the same language as they're asked. Try to generate friendly answers\"\\\n",
    "        f\"\"\"Context: {semantic_search(query=question,\n",
    "                                    embedding_model_name=None,\n",
    "                                    chunk_overlap=0,\n",
    "                                    collection_name = collection_name,\n",
    "                                    documents_limit = 5\n",
    "                                    )}\"\"\"\n",
    "                              ))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf887175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
